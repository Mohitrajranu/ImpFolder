https://synechron.zoom.us/j/97196616584


https://cloudxlab.com/my-lab

mohitrajranu1048
KTYF5H6F

wget https://s3.amazonaws.com/sankethadoop/wordcountnew.jar

wget http://www.gutenberg.org/files/98/98-0.txt

http://scala-ide.org/download/sdk.html

hadoop fs -cp /user/saurzcode/dir1/abc.txt /user/saurzcode/dir2




1. Create a directory in HDFS at given path(s).
1
Usage:
2
hadoop fs -mkdir <paths>

3
Example:
4
create a directory in hdfs
hadoop fs -mkdir hadoop-test1
hadoop fs -mkdir /user/saurzcode/dir1 /user/saurzcode/dir2
2.  List the contents of a directory.
1
Usage :
2
hadoop fs -ls <args>
3
Example:
4
hadoop fs -ls /user/saurzcode
listing root directory
hadoop fs -ls /
listing default to home directory
hadoop fs -ls
hadoop fs -ls /user/ranu
3. Upload and download a file in HDFS.
Upload:

hadoop fs -put:

Copy single src file, or multiple src files from local file system to the Hadoop data file system

1
Usage:
2
hadoop fs -put <localsrc> ... <HDFS_dest_Path>
3
Example:
4
hadoop fs -put /home/saurzcode/Samplefile.txt  /user/saurzcode/dir3/
Download:

hadoop fs -get:

Copies/Downloads files to the local file system

1
Usage:
2
hadoop fs -get <hdfs_src> <localdst>
3
Example:
4
hadoop fs -get /user/saurzcode/dir3/Samplefile.txt /home/
4. See contents of a file
Same as unix cat command:

1
Usage:
2
hadoop fs -cat <path[filename]>
3
Example:
4
hadoop fs -cat /user/saurzcode/dir1/abc.txt
5. Copy a file from source to destination
This command allows multiple sources as well in which case the destination must be a directory.

1
Usage:
2
hadoop fs -cp <source> <dest>
3
Example:
4
hadoop fs -cp /user/saurzcode/dir1/abc.txt /user/saurzcode/dir2
6. Copy a file from/To Local file system to HDFS
copyFromLocal

1
Usage:
2
hadoop fs -copyFromLocal <localsrc> URI
3
Example:
4
hadoop fs -copyFromLocal /home/saurzcode/abc.txt  /user/saurzcode/abc.txt
copy from local fs to hadoop
hadoop fs -copyFromLocal /ranu/test/xyz.csv hadoop-test1

Similar to put command, except that the source is restricted to a local file reference.

copyToLocal

1
Usage:
2
hadoop fs -copyToLocal [-ignorecrc] [-crc] URI <localdst>
Similar to get command, except that the destination is restricted to a local file reference.
copy to local fs from hdfs
hadoop fs -copyToLocal hadoop-test1 /usr/abc.csv
7. Move file from source to destination.
Note:- Moving files across filesystem is not permitted.

1
Usage :
2
hadoop fs -mv <src> <dest>
3
Example:
4
hadoop fs -mv /user/saurzcode/dir1/abc.txt /user/saurzcode/dir2

move a file from one folder to another
hadoop fs -mv hadoop-test1/dwp-payments.csv hadoop-test3
8. Remove a file or directory in HDFS.
Remove files specified as argument. Deletes directory only when it is empty

1
Usage :
2
hadoop fs -rm <arg>
3
Example:
4
hadoop fs -rm /user/saurzcode/dir1/abc.txt
Recursive version of delete.

1
Usage :
2
hadoop fs -rmr <arg>
3
Example:
4
hadoop fs -rmr /user/saurzcode/
9. Display last few lines of a file.
Similar to tail command in Unix.

1
Usage :
2
hadoop fs -tail <path[filename]>
3
Example:
4
hadoop fs -tail /user/saurzcode/dir1/abc.txt
10. Display the aggregate length of a file.
1
Usage :
2
hadoop fs -du <path>
3
Example:
4
hadoop fs -du /user/saurzcode/dir1/abc.txt

check replication
hadoop fs -ls hadoop-test3
change or set replication factor
hadoop fs -Ddfs.replication=2 -cp hadoop-test2/dwp-payments-april.csv hadoop-test2/test_with_rep5.csv
hadoop fs -ls hadoop-test2
hadoop fs -ls hadoop-test2/test_with_rep5.csv
changing permissions
hadoop fs -chmod 777 hadoop-test2/test_with_rep5.csv
File System check - rquires admin priveleges

sudo -u hdfs hdfs fsck /usr/ubuntu/hadoop-test2 -files -blocks -locations

vi /etc/hadoop/conf/hdfs-site.xml
It list property to where the blocks will be physically stored in local system

HDFS Read & Write :: 
A hadoop cluster has 2 types of node ::--> 
In a cluster you can have only one namenode but multiple datanodes.
Namenode[master] => HDFS metadata(list of block locations for a particular file , who created them and when created/modified) and block locations,
suppose a client wants to read a data from hdfs then hdfs fetches list of block locations from namenode , if the replication factor is 3 then 3 datalocations will be returned ,
hdfs will also sort the data locations based on the proximity(Node Proximity --> Nodes that are on the same rack) of the client 

Write operation::
While writing to hdfs , client requests block location from namenode which checks if the client has necessary write permissions , the block location are provided on the basis Replica 
Placement strategy , for replication factor 3  first node will be on the same rack  as client , the other two node will be on different rack say rack2.


Datanode[slave] => Stores actual blocks

MapReduce::
Distributed programming model for large data sets , MapReduce is not a programming language .Hadoop implements mapreduce .MapReduce System(Hadoop) -> Manage communications, data 
transfers parallel execution across distributed servers.
MapReduce is basically carried out in 3 phase:
Map phase --> Input split -->Mappers--> stores data in key value pair
Dataset is divided in to multiple parts - Input Splits, Each mapper process an input split.Each mapper can be called multiple times depending on the content of input split.
Mapper will emit key value pairs as output, there will be one or more mapper in a MapReduce job.

Shuffle phase --> Aggregated the data from multiple source to a final target system.
Reduce phase --> Reducers--> Result
Reduce function will take key value pairs from multiple map functions as input and reduce them to output, keys are grouped with values.Reduce function is called once per key and its values.
There could be 0,1 or more Reduce function for a mapReduce job.
It is advisable to have more than one reducers if you are dealing with multiple mappers.It is important to understand that partitioning happens across all the mappers in the map phase.
Combiner is a mini reducer that runs at map phase. You can use your reducer class as combiner only if your final output with and without combiner is going to remain same.
InputFormat :: Validate inputs ; Input files into logical inputSplits; RecordReader implementation to extract logical records.
OutputFormat :: Validate output specifications [checks if the directory exists or not, will throw error if directory is already there] ; RecordWriter implementation to write output
files of the job.
Writable :: A serializable object which implements a simple,efficient,serialization protocol. Fast,Compact & Effective

create an executable jar and copy it to haddop cluster.
Hdfs input location :: /users/hirw/input/stocks
HDFS output location (relative to user):
output/mapReduce/stocks
hadoop fs -rm -r output/mapReduce/stocks

submit job: hadoop jar 
haddop jar /hive-starterkit/mapReduce/stocks/MaxClosePrice.jar com.hirw.maxcloseprice.MaxClosePrice /users/hirw/input/stocks output/mapReduce/stocks 
view results
haddop fs -cat output/mapReduce/stocks/part-1-00000




1. Create directory "mrinput" on hdfs

hadoop fs -mkdir mrinput


2. Move the book file to HDFS in above directory
hadoop fs -moveFromLocal 98.txt mrinput

3. Execute the below command
hadoop jar wordcountnew.jar wordcountnew <SRC File> mroutput
hadoop jar wordcountnew.jar wordcountnew mrinput/98-0.txt mroutput

4. Download the output in linux
hadoop fs -cat mroutput/part-r-00000
5. View the content of file (processed file)

hadoop fs -ls /apps/hive/warehouse

Apache Pig::
Client tool developed at yahoo, same implementation phases as MapReduce program.
Pig Latin
Simple to use dataflow language, instruction to process data given by the user.Pig analyze,optimize the instructions and construct and submit MR jobs.
PIG Latin instructions :: Load -> Filter ->Group->Aggregate(avg,max)->Store

Philosophy 1:: Pig eats anything(Metadata or not,Structured or not)
Philosophy 2:: Pigs fly[Designed with Big Data performance requirements in mind]
Philosophy 3:: Pigs are domestic Animals(Designed to be easily controlled and modified by users).
Philosophy 4:: Pigs live Anywhere(Pig authors envision pig to work on any distributed processing frameworks similar to hadoop in future)

To interact with pig you need a grunt shell , simply type pig and it will take you the grunt shell in console.
/**************Hadoop In Real World**************/
Hadoop In Real World *** http://www.hadoopinrealworld.com
Pig Latin - Loading & Projecting Datasets
/**************Hadoop In Real World**************/

### LOADING A DATASET ###

grunt> stocks = LOAD '/user/hirw/input/stocks' USING PigStorage(',') as (exchange:chararray, symbol:chararray, date:datetime, open:float, high:float, low:float, close:float,
volume:int, adj_close:float);

delimeter is ',' which is denotated using pigStorage and every column of the stock file is taken into consideration along with there datatype
### STRUCTURE ###
To know the structure of relation we just created above , we will use the describe command desc
grunt> DESC stocks;

### PROJECT AND MANIPULATE FEW COLUMNS FROM DATASET ###

grunt> projection = FOREACH stocks GENERATE symbol, SUBSTRING($0, 0, 1) as sub_exch, close - open as up_or_down;

### PRINT RESULT ON SCREEN ###

grunt> DUMP projection;

### STORE RESULT IN HDFS ###

grunt> STORE projection INTO 'output/pig/simple-projection';

### LOAD 1 - WITH NO COLUMN NAMES AND DATATYPES ###

grunt> stocks = LOAD '/user/hirw/input/stocks' USING PigStorage(',');
describe stocks --> will ask for schema , you can run projection command to generate schems explicitly.

### LOAD 2 - WITH COLUMN NAMES BUT NO DATATYPES ###

grunt> stocks = LOAD '/user/hirw/input/stocks' USING PigStorage(',') as (exchange, symbol, date, open, high, low, close, volume, adj_close);
describe stocks --> will generate schema with default datatype as bytearray.
### LOAD 3 - WITH COLUMN NAMES AND DATATYPES ###

grunt> stocks = LOAD '/user/hirw/input/stocks' USING PigStorage(',') as (exchange:chararray, symbol:chararray, date:datetime, open:float, high:float, low:float, close:float,
volume:int, adj_close:float);

### TO LOOK UP STRUCTURE OF THE RELATION ###

grunt> DESCRIBE stocks;

### WHEN COLUMN NAMES ARE NOT AVAILABLE ###

grunt> projection = FOREACH stocks GENERATE $1 as symbol, SUBSTRING($0, 0, 1) as sub_exch, $6 - $3 as up_or_down;

Pig - Solving a Problem
/**************Hadoop In Real World**************/

grunt> stocks = LOAD '/user/hirw/input/stocks' USING PigStorage(',') as (exchange:chararray, symbol:chararray, date:datetime, open:float, high:float, low:float, close:float,
volume:int, adj_close:float);
 stocks = LOAD '$input' USING PigStorage(',') as (exchange:chararray, symbol:chararray, date:datetime, open:float, high:float, low:float, close:float,
volume:int, adj_close:float);

### FILTERING ONLY RECORDS FROM YEAR 2003 ###

filter_by_yr = FILTER stocks by GetYear(date) == 2003;

### GROUPING RECORDS BY SYMBOL ###

grunt> grp_by_sym = GROUP filter_by_yr BY symbol;
grunt>describe grp_by_sym;
grp_by_sym: {
	group: chararray,
	filter_by_yr: {
		(exchange: chararray,symbol: chararray,date: datetime,open: float,high: float,low: float,close: float,volume: int,adj_close: float)
	}
}

### SAMPLE OUTPUT OF GROUP ###

(CASC, { (NYSE,CASC,2003-12-22T00:00:00.000Z,22.02,22.2,21.94,22.09,36700,20.29), (NYSE,CASC,2003-12-23T00:00:00.000Z,22.15,22.15,21.9,22.05,23600,20.26), ....... })
(CATO, { (NYSE,CATO,2003-10-08T00:00:00.000Z,22.48,22.5,22.01,22.06,92000,12.0), (NYSE,CATO,2003-10-09T00:00:00.000Z,21.3,21.59,21.16,21.45,373500,11.67), ....... })

### CALCULATE AVERAGE VOLUME ON THE GROUPED RECORDS ###

avg_volume = FOREACH grp_by_sym GENERATE group, ROUND(AVG(filter_by_yr.volume)) as avgvolume;

### ORDER THE RESULT IN DESCENDING ORDER ###

avg_vol_ordered = ORDER avg_volume BY avgvolume DESC;

### STORE TOP 10 RECORDS ###

top10 = LIMIT avg_vol_ordered 10;
STORE top10 INTO 'output/pig/avg-volume' USING PigStorage(',');
STORE top10 INTO '$output' USING PigStorage(',');

### EXECUTE PIG INSTRUCTIONS AS SCRIPT ###

pig /hirw-workshop/pig/scripts/average-volume.pig

### PASSING PARAMETERS TO SCRIPT ###

pig -param input=/user/hirw/input/stocks -param output=output/pig/avg-volume-params /hirw-workshop/pig/scripts/average-volume-parameters.pig

### RUNNING A PIG SCRIPT LOCALLY. INPUT AND OUTPUT LOCATION ARE POINTING TO LOCAL FILE SYSTEM ###

pig -x local -param input=/hirw-workshop/input/stocks-dataset/stocks -param output=output/stocks /hirw-workshop/pig/scripts/average-volume-parameters.pig

Pig - Complex Types - Tuple, Bag & Map
/**************Hadoop In Real World**************/

Tuple ->()  is a row and Bag -> {} collection of tuples , collection of rows , bag is more like a table

grunt> stocks = LOAD '/user/hirw/input/stocks' USING PigStorage(',') as (exchange:chararray, symbol:chararray, date:datetime, open:float, high:float, low:float, close:float,
volume:int, adj_close:float);

grunt> filter_by_yr = FILTER stocks by GetYear(date) == 2003;

### STRUCTURE OF A GROUP BY RESULTSET ###

grunt> grp_by_sym = GROUP filter_by_yr BY symbol;
describe grp_by_sym;
grp_by_sym: {
	group: chararray,
	filter_by_yr: {
		(exchange: chararray,symbol: chararray,date: datetime,open: float,high: float,low: float,close: float,volume: int,adj_close: float)
	}
}
grunt>limit10 = LIMIT grp_by_sym 10;
grunt>DUMP limit10;
(CASC, { (ABCSE,CASC,2003-12-22T00:00:00.000Z,22.02,22.2,21.94,22.09,36700,20.29), (ABCSE,CASC,2003-12-23T00:00:00.000Z,22.15,22.15,21.9,22.05,23600,20.26), ....... })
(CATO, { (ABCSE,CATO,2003-10-08T00:00:00.000Z,22.48,22.5,22.01,22.06,92000,12.0), (ABCSE,CATO,2003-10-09T00:00:00.000Z,21.3,21.59,21.16,21.45,373500,11.67), ....... })


### PROJECTING NESTED COLUMNS ###

close_by_sym = FOREACH grp_by_sym GENERATE group, filter_by_yr.close;

(CASC,{(19.5),(15.76),(15.73),(15.75),(15.8),(15.55),(15.85),(15.9),(15.85),(15.94),(16.16),(16.06),(15.84),(15.79),(15.75),........})
(CATO,{(22.06),(21.45),(20.9),(21.33),(21.0),(20.94),(21.05),(20.98),(20.68),(20.69),(20.14),(20.1),(19.95),(20.64),(20.94),........})

### FINDING MAX CLOSE PRICE BY SYMBOL ###

max_close_by_sym = FOREACH grp_by_sym GENERATE group, MAX(filter_by_yr.close);

(BBVA,13.85)
(BRFS,17.35)
(CACI,52.03)
(CASC,27.39)
(CATO,25.11)

### GROUPING BY MORE THAN ONE COLUMN ###

grunt> grp_by_sym_yr = GROUP stocks BY (symbol, GetYear(date));


grunt> DESCRIBE grp_by_sym_yr;
grp_by_sym_yr: {
	group: (symbol: chararray, org.apache.pig.builtin.getyear_date_123: int),
	stocks: {
		(exchange: chararray,symbol: chararray,date: datetime,open: float,high: float,low: float,close: float,volume: int,adj_close: float)
	}
}


grunt> max_close_by_sym_yr = FOREACH grp_by_sym_yr GENERATE group, MAX(stocks.close);

((CATO,2001),21.75)
((CATO,2002),27.44)
((CATO,2003),25.11)
((CATO,2004),29.44)
((CATO,2005),33.26)
((CATO,2006),26.25)
((CATO,2007),25.01)
((CATO,2008),19.38)
((CATO,2009),22.86)
((CATO,2010),21.84)
((CHSP,2010),19.25)
((CLNY,2009),20.75)
((CLNY,2010),20.99)
((DEXO,2010),34.4)
((DOLE,2009),12.5)
((DOLE,2010),12.44)

### PROJECT INDIVIDUAL COLUMNS FROM GROUP ###

grunt> max_close_by_sym_yr = FOREACH grp_by_sym_yr GENERATE group.symbol, group.$1, MAX(stocks.close);

DUMP max_close_by_sym_yr;

### STRUCTURE OF GROUP RESULT BY TWO COLUMNS ###

--If you like the columns to be delimited by comma - by default delimiter is comma
grunt> STORE grp_by_sym_yr INTO 'output/pig/grp_by_sym_yr';

--If you like the columns to be delimited by comma - by default delimiter is comma
grunt> STORE grp_by_sym_yr INTO 'output/pig/grp_by_sym_yr' USING PigStorage(',') ;

(DOLE,2010)     {(ABCSE,DOLE,2010-02-05T00:00:00.000Z,11.1,11.14,10.75,10.99,827400,10.99),(ABCSE,DOLE,2010-02-04T00:00:00.000Z,11.38,11.43,11.02,11.15,510000,11.15),(ABCSE,DOLE,2010-02-03T00:00:00.000Z,11.26,11.58,11.26,11.34,337600,11.34),(ABCSE,DOLE,2010-02-02T00:00:00.000Z,11.77,11.86,11.33,11.35,737600,11.35),(ABCSE,DOLE,2010-02-01T00:00:00.000Z,11.59,11.88,11.48,11.86,463400,11.86),(ABCSE,DOLE,2010-01-29T00:00:00.000Z,11.61,11.65,11.41,11.5,472000,11.5),(ABCSE,DOLE,2010-01-28T00:00:00.000Z,11.94,11.94,11.51,11.62,396700,11.62),(ABCSE,DOLE,2010-01-27T00:00:00.000Z,11.82,11.94,11.66,11.88,312900,11.88),(ABCSE,DOLE,2010-01-26T00:00:00.000Z,11.65,12.02,11.63,11.84,400200,11.84),(ABCSE,DOLE,2010-01-25T00:00:00.000Z,11.83,11.88,11.61,11.74,331400,11.74),(ABCSE,DOLE,2010-01-22T00:00:00.000Z,12.13,12.13,11.7,11.8,416200,11.8),(ABCSE,DOLE,2010-01-21T00:00:00.000Z,12.43,12.43,12.01,12.1,511800,12.1),(ABCSE,DOLE,2010-01-20T00:00:00.000Z,12.32,12.45,12.2,12.44,487700,12.44),(ABCSE,DOLE,2010-01-19T00:00:00.000Z,12.25,12.33,12.19,12.33,262800,12.33),(ABCSE,DOLE,2010-01-15T00:00:00.000Z,12.41,12.42,12.11,12.2,228600,12.2),(ABCSE,DOLE,2010-01-14T00:00:00.000Z,12.25,12.41,12.2,12.41,1442800,12.41),(ABCSE,DOLE,2010-01-13T00:00:00.000Z,12.15,12.32,12.08,12.28,279700,12.28),(ABCSE,DOLE,2010-01-12T00:00:00.000Z,12.04,12.23,12.01,12.16,267800,12.16),(ABCSE,DOLE,2010-01-11T00:00:00.000Z,12.1,12.15,11.92,12.11,725800,12.11),(ABCSE,DOLE,2010-01-08T00:00:00.000Z,12.2,12.29,11.78,12.0,964100,12.0),(ABCSE,DOLE,2010-01-07T00:00:00.000Z,12.26,12.27,12.01,12.25,801200,12.25),(ABCSE,DOLE,2010-01-06T00:00:00.000Z,12.14,12.21,12.01,12.21,640800,12.21),(ABCSE,DOLE,2010-01-05T00:00:00.000Z,12.06,12.24,12.03,12.13,280800,12.13),(ABCSE,DOLE,2010-01-04T00:00:00.000Z,12.35,12.45,12.02,12.04,269400,12.04),(ABCSE,DOLE,2010-02-08T00:00:00.000Z,11.01,11.01,10.81,10.85,396700,10.85)}

### DEFINING A PIG RELATION USING COMPLEX TYPES ###

grunt> complex_type = LOAD 'output/pig/grp_by_sym_yr' as  (grp:tuple(sym:chararray, yr:int), stocks_b:{stocks_t:(exchange:chararray, symbol:chararray, date:datetime, open:float, high:float, low:float, close:float, volume:int, adj_close:float)}) ;


grunt> max_vol_by_yr = FOREACH complex_type GENERATE grp.sym, grp.yr, MAX(stocks_b.volume);
grunt> limit10 = LIMIT max_vol_by_yr 10;
grunt> DUMP limit10;

(B3,1990,456000)
(B3,1991,462000)
(B3,1992,579000)
(B3,1993,1003600)
(B3,1994,1294000)
(B3,1995,551200)
(B3,1996,882800)
(B3,1997,632400)
(B3,1998,540000)
(B3,1999,1054000)


### COMPLEX TYPE - MAP ###
Map is denoted by [] and key , values are seperated by #
328;ADMIN HEARNG;[street#939 W El Camino,city#Chicago,state#IL]
43;ANIMAL CONTRL;[street#415 N Mary Ave,city#Chicago,state#IL]

grunt> departments = LOAD '/user/hirw/input/employee-pig/department_dataset_chicago' using PigStorage(';') AS (dept_id:int, dept_name:chararray, address:map[]);

grunt> dept_addr = FOREACH departments GENERATE dept_name, address#'street' as street, address#'city' as city, address#'state' as state;

PIG Joins::
/**************Hadoop In Real World**************/
/user/hirw/input/dividends-> Directory having a file called as dividends
### LOAD stocks ###

grunt> stocks = LOAD '/user/hirw/input/stocks' USING PigStorage(',') as (exchange:chararray, symbol:chararray, date:datetime, open:float, high:float, low:float, close:float,
volume:int, adj_close:float);

### LOAD dividends ###

grunt> divs = LOAD '/user/hirw/input/dividends' USING PigStorage(',') as (exchange:chararray, symbol:chararray, date:datetime, dividends:float);
grunt> DESCRIBE divs;
divs: 
	{
		exchange: chararray,
		symbol: chararray,
		date: datetime,
		dividends: float
	}


### INNER JOIN ###

grunt> join_inner = JOIN stocks BY (symbol, date) , divs BY (symbol, date);	

grunt> DESCRIBE join_inner;

join_inner: {
	stocks::exchange: chararray,
	stocks::symbol: chararray,
	stocks::date: datetime,
	stocks::open: float,
	stocks::high: float,
	stocks::low: float,
	stocks::close: float,
	stocks::volume: int,
	stocks::adj_close: float,
	divs::exchange: chararray,
	divs::symbol: chararray,
	divs::date: datetime,
	divs::dividends: float
	}

grunt> join_project  = FOREACH join_inner GENERATE stocks::symbol, divs::date, divs::dividends;

grunt> DUMP join_project;

### LEFT OUTER ###

grunt> join_left = JOIN stocks BY (symbol, date) LEFT OUTER, divs BY (symbol, date);

grunt> DUMP join_left;

--Filter out records with no dividends
grunt> filterleftjoin = FILTER join_left BY divs::symbol IS NOT NULL;

grunt> DUMP filterleftjoin;


### RIGHT OUTER ###

grunt> join_right = JOIN stocks BY (symbol, date) RIGHT OUTER, divs BY (symbol, date);

grunt> DUMP join_right;


### FULL JOIN ###

--FULL OUTER will display rows from both sides matched and unmatched. Combination of LEFT OUTER and RIGHT OUTER
grunt> join_full = JOIN stocks BY (symbol, date) FULL, divs BY (symbol, date);

grunt> DUMP join_full;


### Multiway Join ###


### LOAD companies ###	

companies = LOAD '/user/hirw/input/companies' USING PigStorage(';') as (symbol:chararray, name:chararray, address: map[]);

cmp = FOREACH companies GENERATE symbol, name, address#'street', address#'city', address#'state';


grunt> join_multi = JOIN stocks by (symbol, date), divs by (symbol, date), cmp by symbol; --> invalid instructions

--Multiway join is only possible on inner joins and not on outer joins and join key will be same for all the 3 tables.
grunt> join_multi = JOIN stocks by symbol, divs by symbol, cmp by symbol;

grunt> DUMP join_multi;

###CROSS###

--Takes every record in stocks and combines it with every record in divs
grunt> crs = CROSS stocks, divs;

--What about non equi joins? --> It is very hard to implement non-eqi joins using map-reduce , hence there is no concrete latin pig instruction for it. So we overcome this using cross join
grunt> non_equi = FILTER crs by stocks::symbol != divs::symbol;

grunt> limit1000 = LIMIT non_equi 1000; 


###COGROUP### join without using join operator, cogroup will provide a nested output.

grunt> cgrp = COGROUP stocks BY (symbol, date), divs by (symbol, date);//similar to full outer join.

cgrp: {
	group: (symbol: chararray,date: chararray),
	stocks: {(exchange: chararray,symbol: chararray,date: chararray,open: float,high: float,low: float,close: float,volume: int,adj_close: float)},
	divs: {(exchange: bytearray,symbol: bytearray,date: bytearray,dividends: bytearray)}
	}



((CSL,2009-05-14),{},{(ABCSE,CSL,2009-05-14,0.155)})
((CSL,2009-08-12),{(ABCSE,CSL,2009-08-12,32.65,32.73,32.17,32.54,528900,32.39)},{(ABCSE,CSL,2009-08-12,0.16)})
((CSL,2009-08-13),{(ABCSE,CSL,2009-08-13,32.58,33.19,32.49,33.15,447600,32.99)},{})

grunt> filter_empty_divs = FILTER cgrp BY (NOT IsEmpty(stocks)) AND  (NOT IsEmpty(divs)); //similar to inner join.

grunt> limit10 = LIMIT filter_empty_divs 10;

grunt> DUMP limit10;


filter_empty_divs = FILTER cgrp BY (IsEmpty(group));
limit10 = LIMIT filter_empty_divs 10;
DUMP limit10;


wget https://s3.amazonaws.com/sankethadoop/All_States_PinCode.csv


create database <USERNAME>;

use <USERNAME>;

show tables;

CREATE TABLE temp_India (
OFFICE_NAME STRING,
OFFICE_STATUS STRING,
PINCODE INT,
TELEPHONE STRING,
TALUK STRING,
DISTRICT STRING,
STATE STRING,
POSTAL_DIVISION STRING,
POSTAL_REGION STRING,
POSTAL_CIRCLE STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '/home/cloudera/data/All_States_PinCode.csv' INTO TABLE temp_India;

select * from temp_india limit 1;

CREATE TABLE India (
OFFICE_NAME STRING,
OFFICE_STATUS STRING,
PINCODE INT,
TELEPHONE STRING,
TALUK STRING,
DISTRICT STRING,
POSTAL_DIVISION STRING,
POSTAL_REGION STRING,
POSTAL_CIRCLE STRING
)
PARTITIONED BY (STATE STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE;

SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;

INSERT OVERWRITE TABLE India PARTITION (STATE)
SELECT
OFFICE_NAME ,
OFFICE_STATUS ,
PINCODE ,
TELEPHONE ,
TALUK ,
DISTRICT ,
POSTAL_DIVISION ,
POSTAL_REGION ,
POSTAL_CIRCLE ,
STATE
FROM temp_India;

-m 1 --> table doesnot have primary key , mapper > 1 requires primary key in the table.

spark-shell

aNotepad.com - free online notepad
Home
Features
About
Register/Login
commands
Share

 
  // VALUES are immutable constants. You can't change them once defined.
  val hello: String = "Hello!"                    //> hello  : String = Hello!
  println(hello)                                  //> Hello!
  
  // Notice how Scala defines things backwards from other languages - you declare the
  // name, then the type.
  
  // VARIABLES are mutable
  var helloThere: String = hello                  //> helloThere  : String = Hello!
  helloThere = hello + " There!"
  println(helloThere)                             //> Hello! There!
  
  
  // One key objective of functional programming is to use immutable objects as often as possible.
  // Try to use operations that transform immutable objects into a new immutable object.
  // For example, we could have done the same thing like this:
  val immutableHelloThere = hello + "There!"      //> immutableHelloThere  : String = Hello!There!
  println(immutableHelloThere)                    //> Hello!There!
  
  // Some other types
  val numberOne : Int = 1                         //> numberOne  : Int = 1
  val truth : Boolean = true                      //> truth  : Boolean = true
  val letterA : Char = 'a'                        //> letterA  : Char = a
  val pi : Double = 3.14159265                    //> pi  : Double = 3.14159265
  val piSinglePrecision : Float = 3.14159265f     //> piSinglePrecision  : Float = 3.1415927
  val bigNumber : Long = 1234567890l              //> bigNumber  : Long = 1234567890
  val smallNumber : Byte = 127                    //> smallNumber  : Byte = 127
  
  // String printing tricks
  // Concatenating stuff with +:
  println("Here is a mess: " + numberOne + truth + letterA + pi + bigNumber)
                                                  //> Here is a mess: 1truea3.141592651234567890
  
  // printf style:
  println(f"Pi is about $piSinglePrecision%.3f")  //> Pi is about 3.142
  println(f"Zero padding on the left: $numberOne%05d")
                                                  //> Zero padding on the left: 00001
                                                  
  // Substituting in variables:
  println(s"I can use the s prefix to use variables like $numberOne $truth $letterA")
                                                  //> I can use the s prefix to use variables like 1 true a
  // Substituting expressions (with curly brackets):
  println(s"The s prefix isn't limited to variables; I can include any expression. Like ${1+2}")
                                                  //> The s prefix isn't limited to variables; I can include any expression. Like
                                                  //|  3
                                                 
  // Using regular expressions:
  val theUltimateAnswer: String = "To life, the universe, and everything is 42."
                                                  //> theUltimateAnswer  : String = To life, the universe, and everything is 42.
                                                  //| 
  // Go and look for first number in the string
  // \d+ means look for integers and .* means the first interger in row
  val pattern = """.* ([\d]+).*""".r              //> pattern  : scala.util.matching.Regex = .* ([\d]+).*
  val pattern(answerString) = theUltimateAnswer   //> answerString  : String = 42
  val answer = answerString.toInt                 //> answer  : Int = 42
  println(answer)                                 //> 42
    
  // Dealing with booleans
  val isGreater = 1 > 2                           //> isGreater  : Boolean = false
  val isLesser = 1 < 2                            //> isLesser  : Boolean = true
  val impossible = isGreater & isLesser           //> impossible  : Boolean = false
  
  val picard: String = "Picard"                   //> picard  : String = Picard
  val bestCaptain: String = "Picard"              //> bestCaptain  : String = Picard
  val isBest: Boolean = picard == bestCaptain     //> isBest  : Boolean = true
  
  // EXERCISE
  // Write some code that takes the value of pi, doubles it, and then prints it within a string with
  // three decimal places of precision to the right.
  // Just write your code below here; any time you save the file it will automatically display the results!
  
}

 
// Flow control
  
  // If / else syntax
  if (1 > 3) println("Impossible!") else println("The world makes sense.")
                                   
  
  if (1 > 3) {
  	println("Impossible!")
  } else {
  	println("The world makes sense.")
  }                                              
  
  // Matching - like switch in other languages and _ refers to default:
  val number = 3                                
  number match {
  	case 1 => println("One")
  	case 2 => println("Two")
  	case 3 => println("Three")
  	case _ => println("Something else")
 	}                                        
 	
 	// For loops - <- is a range operator
 	for (x <- 1 to 4) {
 		val squared = x * x
 		println(squared)
 	}                                        
                                                  
  // While loops
  var x = 10                                   
  while (x >= 0) {
  	println(x)
  	x -= 1
  }                                               
                                                  
  x = 0
  do { println(x); x+=1 } while (x <= 10)      
                                                  
                                                  
   // Expressions
   // "Returns" the final value in a block automatically
      {val x = 10; x + 20}                          
                                                
	 println({val x = 10; x + 20})            
	 
	 // EXERCISE
	 // Write some code that prints out the first 10 values of the Fibonacci sequence.
	 // This is the sequence where every number is the sum of the two numbers before it.
	 // So, the result should be 0, 1, 1, 2, 3, 5, 8, 13, 21, 34
	 
	   
}

pyspark
sparkR


1. Install a JDK (Java Development Kit) from http://www.oracle.com/technetwork/java/javase/downloads/index.html . Keep track of where you installed the JDK; you’ll need that later.

2. Download a pre-built version of Apache Spark from https://spark.apache.org/downloads.html

3. If necessary, download and install WinRAR so you can extract the .tgz file you downloaded. http://www.rarlab.com/download.htm

4. Extract the Spark archive, and copy its contents into C:\spark after creating that directory. You should end up with directories like c:\spark\bin, c:\spark\conf, etc.

5. Move winutils.exe (Download from - https://s3.amazonaws.com/sankethadoop/winutils.exe) into a C:\winutils\bin folder that you’ve created. (note, this is a 64-bit application. If you are on a 32-bit version of Windows, you’ll need to search for a 32-bit build of winutils.exe for Hadoop.)

6. Open the the c:\spark\conf folder, and make sure “File Name Extensions” is checked in the “view” tab of Windows Explorer. Rename the log4j.properties.template file to log4j.properties. Edit this file (using Wordpad or something similar) and change the error level from INFO to ERROR for log4j.rootCategory

7. Right-click your Windows menu, select Control Panel, System and Security, and then System. Click on “Advanced System Settings” and then the “Environment Variables” button.

8. Add the following new USER variables:
a. SPARK_HOME c:\spark
b. JAVA_HOME (the path you installed the JDK to in step 1, for example C:\Program Files\Java\jdk1.8.0_101)
c. HADOOP_HOME c:\winutils

9. Add the following paths to your PATH user variable: %SPARK_HOME%\bin; %JAVA_HOME%\bin

10. Close the environment variable screen and the control panels.

11. Install the latest Scala IDE from http://scala-ide.org/download/sdk.html

12. Test it out!
a. Open up a Windows command prompt in administrator mode.
b. Enter cd c:\spark and then dir to get a directory listing.
c. Look for a text file we can play with, like README.md or CHANGES.txt
d. Enter spark-shell
e. At this point you should have a scala> prompt. If not, double check the steps above.
f. Enter val rdd = sc.textFile(“README.md”) (or whatever text file you’ve found)
g. Enter rdd.count()
h. You should get a count of the number of lines in that file! Congratulations, you just ran your first Spark program!
i. Hit control-D to exit the spark shell, and close the console window
j. You’ve got everything set up! Hooray!

 
 built.sbt
name := "RatingsCounter"

version := "1.0"

scalaVersion := "2.11.8"

libraryDependencies ++= Seq(
"org.apache.spark" %% "spark-core" % "2.1.1" % "provided"
)

project/build.properties
sbt.version=1.2.8

RatingsCounter.scala
import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.log4j._
//count number of movies for each rating
object RatingsCounter {

//main func
def main(args: Array[String]) {

//set log level
//Logger.getLogger("org").setLevel(Level.ERROR)

//create SparkContext
//val sc = new SparkContext("local[*]","RatingsCounter")
val conf = new SparkConf().setAppName("RatingsCounter")
val sc = new SparkContext(conf)
//load data
val lines = sc.textFile("hdfs://cxln1.c.thelab-240901.internal:8020/user/sanketthodge8502/sparkdata/u.data")

//extract 3rd column
val ratings = lines.map(x => x.toString().split("\t")(2))

//count
val results = ratings.countByValue()

//sorting
val sortedresults = results.toSeq.sortBy(_._1)

//print output
sortedresults.foreach(println)
} }

sbt

compile

package

spark-submit <PATH TO JAR RECEIVED AFTER EXECUTING PREVIOUS COMMAND>

