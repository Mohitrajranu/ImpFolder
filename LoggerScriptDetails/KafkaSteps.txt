https://docs.spring.io/spring-kafka/reference/html/

kafka doesnot parse or even read your data (zero cpu usage) , kafka takes byte as an input without even loading them into memory (zero copy). It distributes bytes it doesn't know if your data is an Integer, String etc.
Use Confluent Schema registry with avro as datasource.

https://medium.com/@stephane.maarek/the-kafka-api-battle-producer-vs-consumer-vs-kafka-connect-vs-kafka-streams-vs-ksql-ef584274c1e



######################################################################################################################################################
//add shutdownhook

Runtime.getRuntime().addShutdownHook(new Thread(() -> {
logger.info("Stopping application");
client.stop();
producer.close();
logger.info("Stopping producer");
}));

*******************************************************************************************************************************************************
kafka-console-producer --broker-list 127.0.0.1:9092 --topic first_topic
kafka-console-producer --broker-list 127.0.0.1:9092 --topic first_topic --producer-property acks=all
Producer with keys
kafka-console-producer --broker-list 127.0.0.1:9092 --topic first_topic --property parse.key=true --property key.seperator=,
>key,value Mark,salary : 900

kafka-topics --zookeeper 127.0.0.1:2181 --list
kafka-topics --zookeeper 127.0.0.1:2181 --topic topicname --describe
kafka-topics --zookeeper 127.0.0.1:2181 --topic topicname --describe __consumer_offsets
kafka-console-consumer --bootstrap-server 127.0.0.1:9092 --topic first_topic
Consumer with keys
kafka-console-consumer --bootstrap-server 127.0.0.1:9092 --topic first_topic --from-beginning --property print.key=true --property key.seperator=,

kafka-console-consumer --bootstrap-server 127.0.0.1:9092 --topic first_topic --from-beginning
kafka-console-consumer --bootstrap-server 127.0.0.1:9092 --topic first_topic --group groupName
kafka-consumer-groups --bootstrap-server localhost:9092 --list
kafka-consumer-groups --bootstrap-server localhost:9092 --describe --group groupName
kafka-consumer-groups --bootstrap-server localhost:9092 --group my-first-application --reset-offsets --to-earliest --execute --topic first_topic
	
kafka-consumer-groups --bootstrap-server localhost:9092 --group my-first-application --reset-offsets --shift-by -2 --execute --topic first_topic

kafka-topics --zookeeper 127.0.0.1:2181 --create --topic employee-salary --partitions 1 --replication-factor 1 --config cleanup.policy=compact --config min.cleanable.dirty.ratio=0.001 --config segment.ms=5000
kafka-configs --zookeeper 127.0.0.1:2181 --entity-type topics --entity-name configured-topic --add-config min.insync.replicas=2 --alter //--delete-config
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Each partition has its own leader, every partition is distributed across multiple brokers, Producers write data to topics they automatically know to which broker and partition to write to in case of broker failures producer will automatically recover the load is balanced to many brokers thanks to the number of partitions.Producers can choose to send a key with messsage if key is null data is sent round robin(broker 101,broker 102,broker 103), if a key is sent then all the messages for that key will always go to the same partition, a key is basically sent if you need message ordering for a specific field eg truck_id.Consumers read data in consumer groups each consumer within a group reads from exclusive partitions.If you have more than partitions some consumers will be inactive, Consumers will automatically use a groupCoordinator to assign a consumers to a partition.
Kafka stores the offsets at which consumer group has been reading, the offset committed live in a kafka topic named _consumer_offsets.
Consumers choose when to commit offsets, there are 3 delivery semantics:
1]At most once: offsets are committed as soon as the message is received, if the processing goes wrong the message will be lost(won't be read again).
2]At Least Once(usually preferred): offsets are committed after the message is processed,if the processing goes again message will be reaad again., This can result in duplicate processing of messages , make sure your processing  is idempotent.
3]Exactly once:Can be achieved for kafka -> Kafka workflows using kafka stream api, For kafka external system workflows, use an idempotent consumer.

Kafka Broker Discovery:: Every kafka broker is called a bootstrap server, that means you only need to connect to one broker, and you will be connected to the entire cluster. Each broker knows about all brokers , topics and partitions (metadata).


As you wrote, Kafka guarantees ordered delivery only within a single partition. Period. If you are using multiple partitions (which is a must to have the parallelism), then it is possible that a consumer who listens on several partitions gets a message A from partition 1 before a message B from partition 2, even though message B arrived first.

Now, about the differences between Kafka and JMS. In JMS, you have a queue and you have a topic. With queues, when first consumer consumes a message, others cannot take it anymore. With topics, multiple consumers receive each message but it is much harder to scale. Consumer group from Kafka is a generalization of these two concepts - it allows scaling between members of the same consumer group, but it also allows broadcasting the same message between many different consumer groups.

Even more important difference is the following. Imagine that you have Kafka topic with 500 partitions and on the other hand, 500 JMS message queues. Let's also imagine that you have certain number of producers and consumers. In case of JMS, you need to configure each of them so they know which queues belong to them. What if e.g. some consumer crashes or you detect that you need to increase number of consumers? You have to reconfigure manually the whole system. This comes for free with Kafka, i.e. Kafka provides automatic rebalancing which is an extremely useful feature.

Finally, Kafka is tremendously faster, mostly because of some clever disk/memory transfer techniques and because consumers take care about the messages they consumed, not the broker like in JMS. Because of this, consumer is also able to "rewind", i.e. reread the messages from e.g. 2 days ago.

In Kafka the parallelism is equal to the number of partitions for a topic.

For example, assume that your messages are partitioned based on user_id and consider 4 messages having user_ids 1,2,3 and 4. Assume that you have an "users" topic with 4 partitions.

Since partitioning is based on user_id, assume that message having user_id 1 will go to partition 1, message having user_id 2 will go to partition 2 and so on..

Also assume that you have 4 consumers for the topic. Since you have 4 consumers, Kafka will assign each consumer to one partition. So in this case as soon as 4 messages are pushed, they are immediately consumed by the consumers.

If you had 2 consumers for the topic instead of 4, then each consumer will be handling 2 partitions and the consuming throughput will be almost half.

To completely answer your question, Kafka only provides a total order over messages within a partition, not between different partitions in a topic.

ie, if consumption is very slow in partition 2 and very fast in partition 4, them message with user_id 4 will be consumed before message with user_id 2. This is how Kafka is designed.

In kafka Messages with the same key, from the same Producer, are delivered to the Consumer in order

another thing on top of that is, Data within a Partition will be stored in the order in which it is written therefore, data read from a Partition will be read in order for that partition

So if you want to get your messages in order across multi partitions, then you really need to group your messages with a key, so that messages with same key goes to same partition and with in that partition the messages are ordered.

In a nutshell, you will need to design a two level solution like above logically to get the messages ordered across multi partition.
When a Consumer joins a consumer group it will fetch the last committed offset so it will restart to read from 5,6,7 if before crashing it committed the latest offset(so 4). The earliest and latest values for the auto.offset.reset property is used when a consumer starts but there is no committed offset for the assigned partition.In this case you can choose if you want to re-read all the messages from the beginning(earliest) or just after the last one(latest).
@@@@@@@@@@@@@@@@@@@@@@@@@@@ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
Kafka Connect:: Simplify and improve getting data in and out of Kafka.It allows you to have a source where all its data will be put.

Kafka connect source api: Ex --> Get data from a file into a kafka topic(FileStreamSourceConnector) or Get data from twitter into a kafka topic (TwitterSourceConnector).
Kafka connect sink api: Ex --> Store the data from kafka into Elastic Search (ElasticSearchSinkConnector) or Store the data from kafka into PostgreSql(JDBCSinkConnector).
Source Connectors to get data from common data sources.Sink connectors to publish that data in common data stores.Re-usable code only configurational changes.
Kafka Connect Cluster has multiple loaded Connectors.Each connector is a re-usable piece of code(java jars).Connectors + User Configuration =>Tasks. A task is linked to a connector configuration. A Job configuration may spawn multiple tasks. Tasks are executed by Kafka connect Workers(Servers). A worker is a single java process. A worker can be standalone or in a cluster.
##Configuring worker.properties in standalone mode.
bootstrap-servers=127.0.0.1:9092
key.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=false
value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable=false
#We always leave the internal key to JsonConverter.
internal.key.converter=org.apache.kafka.connect.json.JsonConverter
internal.key.converter.schemas.enable=false
internal.value.converter=org.apache.kafka.connect.json.JsonConverter
internal.value.converter.schemas.enable=false
# Rest Api
rest.port=8086
rest.host.name=127.0.0.1
# This Config is only for standalone workers.
offset.storage.file.filename=standalone.offsets
offset.flush.interval.ms=10000
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
List of available connectors:- There are many source and sink connectors , you can find them in confluent website https://www.confluent.io/hub/
TwitterSourceConnector:
Goal---> gather data from twitter in kafka connect distributed mode.
Twitter------------>Kafka Connect----------->twitter topic.
Learning:: Gather real data using https://github.com/Eneco/kafka-connect-twitter
Create twitter app and generate access token.
Go to connect Ui and apply the configuration.
#The name which you give to the kafka connector.It is used for offset management, groupId and so on.
name=source-twitter-distributed
#It actually tells kafka connect what class to run as source connector.
connector.class=com.enco.trading.kafka.connect.twitter.TwitterSourceConnector
#No of task you run in parallel to complete your job.
tasks.max=1
topic=demo-3-twitter
key.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=true
value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable=true
#Twitter connector specific configuration 
twitter.consumerkey=9jT77YU0Orhgqsg
twitter.consumersecret=6utru27yi2g
twitter.token=1jvhnvk17816bvx
twitter.secret=thf12468gwsy87
track.terms=programming,java,kafka,scala,usa,rihana
language=en 
1.create topic
# Kafka command lines tools
docker run --rm -it --net=host landoop/fast-data-dev bash
kafka-topics --create --topic demo-3-twitter --zookeeper 127.0.0.1:2181 --partitions 3 --replication-factor 1
kafka-console-consumer --topic demo-3-twitter --bootstrap-server 127.0.0.1:9092

We can consider elastisearch instance to sink the data , they will serve as our sink for first connector, elasticsearch is an easy way to store json data and search across it.

ElasticSearchSinkConnector Distributed Mode
Goal: Start an elasticSearch instance using docker , sink a topic with multiple partitions to elasticsearch , run in distributed mode with multiple tasks 

Kafka Twitter Topic ------------------> ElasticSearchSinkConnector ----------------->ElasticSearch.

Learn about the tasks.max parameter , understand how sink connectors work.

docker-compose.yml
docker-compose up kafka-cluster elasticsearch postgres

version: '2'
services: 
  #This is our kafka cluster.
  kafka-cluster:
       image: landoop/fast-data-dev: latest
       environment:
          ADV_HOST: 127.0.0.1
          RUNTESTS: 0 # disable run tests so the cluster starts faster.
       ports:
         - 2181:2181  #ZooKeeper
		 - 3030:3030  #Landoop UI
		 - 8081-8083:8081-8083  # REST Proxy,Schema Registry, kafka connect ports
		 - 9581-9585:9581:9585  # JMX Ports
		 - 9092:9092
	# This configuration allows you to start elasticsearch	 
   elasticsearch:
       image: itag/elasticsearch:2.4.3
       environment:
	     PLUGINS:appbaseio/dejavu
		 OPTS: -Dindex.number_of_shards=1 -Dindex.number_of_replicas=0
	   ports:
	     - "9200:9200"
	# This configuration allows you to start elasticSearch	 
   postgres:
      image: postgres:9.5-alpine
	  environment:
	    POSTGRES_USER: postgres
		POSTGRES_PASSWORD: postgres
		POSTGRES_DB: postgres
	   ports:
          - 5432:5432	   
 
# Visualize the data at:
http://127.0.0.1:9200/_plugin/dejavu
Click on add query::
Name :: High Friends Count
Type :: kafka-connect
Query body:: 
{
   "query":{
    "range" :{
	 "user.friends_count" :{
	     "gt" : 500
	 }
	}
   }
}

{
   "query":{
    "term" :{
	 "is_retweet" : true
	}
   }
}

# Counting number of tweets:
http://127.0.0.1:9200/demo-3-twitter/_count

sink-elastic-twitter-distributed.properties

#Basic Configuration for our connector.
name=sink-elastic-twitter-distributed
connector.class=io.confluent.connect.elasticsearch.ElasticsearchSinkConnector
# We can have parallelism here so we have two tasks!
tasks.max=2
topics=demo-3-twitter
key.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=true
value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable=true
#ElasticSearch connector specific configuration
connection.url=http://elasticsearch:9200 #http://docs.confluent.io/3.1.1/connect/connect-elasticsearch/docs/configuration_options.html
type.name=kafka-connect # table name
key.ignore= true
#because our keys from the twitter feed are null.

********************************************************************************************************************************************************
Kafka Connect REST Api
apk update && apk add jq
1. All the actions performed by landoop kafka connect UI are actually triggering REST API calls to kafka connect.
http://docs.confluent.io/3.2.0/connect/managing.html#common-rest-examples
1) Get Worker information ::: curl -s 127.0.0.1:8083/ | jq
2)List connectors available on a worker ::: curl -s 127.0.0.1:8083/connector-plugins | jq
3) Active connectors ::: curl -s 127.0.0.1:8083/connectors | jq
4) Get information about a connector tasks and config curl -s 127.0.0.1:8083/connectors/source-twitter-distributed/tasks | jq
5) Get connector Status curl -s 127.0.0.1:8083/connectors/file-stream-demo-distributed/status | jq
6) Pause / Resume a connector (no response if call is successfull)
curl -s -X PUT 127.0.0.1:8083/connectors/file-stream-demo-distributed/pause ; curl -s -X PUT 127.0.0.1:8083/connectors/file-stream-demo-distributed/resume
7)Get connector configuration curl -s 127.0.0.1:8083/connectors/file-stream-demo-distributed | jq
8) Delete our Connector curl -s -X DELETE 127.0.0.1:8083/connectors/file-stream-demo-distributed
9) Create a new connector 
curl -s -X POST -H "Content-Type: application/json" --data '{"name": "file-stream-demo-distributed","config":{"connector.class":"org.apache.kafka.connect.file.FileStreamSourceConnector",
"key.converter.schemas.enable":"true","file":"demo-file.txt", "tasks.max":"1","value.converter.schemas.enable":"true","name": "file-stream-demo-distributed",
"topic":"demo-2-distributed","value.converter":"org.apache.kafka.connect.json.JsonConverter","key.converter":"org.apache.kafka.connect.json.JsonConverter"}}'
 http://127.0.0.1:8083/connectors | jq
 10) Update Connector Configuration
 curl -s -X PUT -H "Content-Type: application/json" --data '{"connector.class":"org.apache.kafka.connect.file.FileStreamSourceConnector",
"key.converter.schemas.enable":"true","file":"demo-file.txt", "tasks.max":"1","value.converter.schemas.enable":"true","name": "file-stream-demo-distributed",
"topic":"demo-2-distributed","value.converter":"org.apache.kafka.connect.json.JsonConverter","key.converter":"org.apache.kafka.connect.json.JsonConverter"}'
127.0.0.1:8083/connectors/file-stream-demo-distributed/config | jq
*************************************************************************************************************************************************************
PostgreSql is a very popular relational database.
Start a postgresql instance using docker,run in distributed mode with multiple tasks, learn about jdbc sink connector.
Kafka Twitter Topic --------------------------->JDBCSinkConnector--------------------------->PostgreSql

^^^^^^^^^^ sink-postgres-twitter-distributed.properties ^^^^^^^^^^^^^^^^
# basic configuration for our connector
name=sink-postgres-twitter-distributed
connector.class=io.confluent.connect.jdbc.JdbcSinkConnector
# We can have parallelism here so we have two tasks!
tasks.max=2
topics=demo-3-twitter
key.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=true
value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable=true
#ElasticSearch connector specific configuration
connection.url=jdbc:postgresql://postgres:5432/postgres
connection.user=postgres
connection.password=postgres
insert.mode=upsert
#we want the  primary key to be offset + partition
pk.mode=kafka
pk.fields=_connect_topic,_connect_offset
field.whitelist=id,created_at,text,lan
auto.create=true
auto.evolve-true.
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
Writing your own connector
The easiest way to get started on a kafka Connect project is to use an archtype provided here:
https://github.com/jcustenborder/kafka-connect-archtype
We will change the kafka dependency to be 0.10.2.0 ( use the one matching the version of landloop)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%55
What is Kafka Streams:- Easy data processing and transformation library within Kafka.Standard Java application no need to create a separate cluster,highly scalable,elastic and fault tolerant.Exactly Once capabilities ,One record at a time processing (no batching) works for any application size. 
Source Partition allows kafka connect to know which source you have been reading , Source offset allows kafka connect to track until when you have been reading for the source partition you have choose.

in connected-distributed.properties add plugin.path=/users/kafka-2.0.11/connectors/

/bin/connect-distributed.sh config/connected-distributed.properties 

for adding connectors plugin , go to debezium.io/install
add jars inside connectors




$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
https://www.cloudera.com/documentation/kafka/latest/topics/kafka_using.html

Kafka------ Run on linux server.

Download Kafka from https://www.apache.org/dyn/closer.cgi?path=/kafka/2.3.0/kafka-2.3.0-src.tgz
 http://mirrors.estointernet.in/apache/kafka/2.2.0/kafka_2.11-2.2.0.tgz
 
extract tar file in the directory say kafka.
cd kafka

nohup bin/zookeeper-server-start.sh  config/zookeeper.properties &


nohup bin/kafka-server-start.sh config/server.properties &
in config/server,properties

add this line listeners = PLAINTEXT://0.0.0.0:9092


//for seeing logs
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic myTopic


If you want to access from LAN, change following 2 files-

In config/server.properties:

advertised.listeners=PLAINTEXT://server.ip.in.lan:9092
In config/producer.properties:

bootstrap.servers=server.ip.in.lan:9092
In my case, the server.ip.in.lan value was 192.168.15.150

# Docker for Mac >= 1.12, Linux, Docker for Windows 10
docker run --rm -it \
           -p 2181:2181 -p 3030:3030 -p 8081:8081 \
           -p 8082:8082 -p 8083:8083 -p 9092:9092 \
           -e ADV_HOST=127.0.0.1 \
           landoop/fast-data-dev

# Docker toolbox
docker run --rm -it \
          -p 2181:2181 -p 3030:3030 -p 8081:8081 \
          -p 8082:8082 -p 8083:8083 -p 9092:9092 \
          -e ADV_HOST=192.168.99.100 \
          landoop/fast-data-dev

For organizing your docker container in a more effective manner you can use docker compose.
		  
docker-compose up kafka-cluster 		  
# Kafka command lines tools
docker run --rm -it --net=host landoop/fast-data-dev bash

# Short instructions for Mac / Linux
# download NiFi at: https://nifi.apache.org/download.html
# Terminal 1:
bin/nifi.sh run
# install docker for mac / docker for linux
# Terminal 2:
docker run -it --rm -p 2181:2181 -p 3030:3030 -p 8081:8081 -p 8082:8082 -p 8083:8083 -p 9092:9092 -e ADV_HOST=127.0.0.1 -e RUNTESTS=0 landoop/fast-data-dev
# Terminal 3:
docker run -it --net=host landoop/fast-data-dev bash
kafka-topics --create --topic nifi-topic --zookeeper 127.0.0.1:2181 --partitions 3 --replication-factor 1
connect-standalone worker.properties file-stream-demo-standalone.properties

FileStreamSourceConnector Distributed mode 

kafka Connect Ui will provide the list of all connector working for you.

kafka-console-consumer --topic nifi-topic --bootstrap-server 127.0.0.1:9092
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Topics and partitions , Topics a particular stream of data(similar to tables in database) a topic is identified by its name. topics are split in partitions, each partition is ordered, each message within a partition gets an incremental id called offset. Offset only have a meaning for specific partition, eg:: offset 3 in partition doesn't represent the same data as offset 3 in partition 1.Order is guaranteed only within a partition not across it,data is kept only for a limited time and is immutable once written cannot be changed.

Kafka cluster is composed of multiple brokers identified by their id.Each broker contains certain topic partitions,a good number to start is 3 brokers.

At any time only one broker can be a leader for a given partition,only that leader can receive and serve data for partition.The other brokers will syncronize the data there each partition has one leader and multiple ISR.

Producer --> send data to a broker(id) , topic name and partition.

Consumer --> specify topic name and broker to connect to,and kafka will automatically take care of pulling the data from right brokers.Data is read in order for each partitions.Consumers read data in consumer groups each consumer within a group reads from exclusive partitions.kafka stores the offset at which consumer group has been reading.The offsets commit live in a kafka topic named "_consumer_offsets".

When a consumer has processed data received from kafka it should be commiting offsets.If a consumer dies it will be able to read back from where it left.
Zookeeper manages brokers, keeps a list of them it helps in performing leader election for partitions. Zookeeper usually operates in odd quorum (clusters) 3,5,7

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Configuring High Availability and Consistency for Apache Kafka
To achieve high availability and consistency targets, adjust the following parameters to meet your requirements:

Replication Factor
Preferred Leader Election
Unclean Leader Election
Acknowledgements
Minimum In-sync Replicas
Kafka MirrorMaker
Replication Factor
The default replication factor for new topics is one. For high availability production systems, Cloudera recommends setting the replication factor to at least three. This requires at least three Kafka brokers.

To change the replication factor, navigate to Kafka Service > Configuration > Service-Wide. Set Replication factor to 3, click Save Changes, and restart the Kafka service.

Preferred Leader Election
Kafka is designed with failure in mind. At some point in time, web communications or storage resources fail. When a broker goes offline, one of the replicas becomes the new leader for the partition. When the broker comes back online, it has no leader partitions. Kafka keeps track of which machine is configured to be the leader. Once the original broker is back up and in a good state, Kafka restores the information it missed in the interim and makes it the partition leader once more.

Preferred Leader Election is enabled by default, and should occur automatically unless you actively disable the feature. Typically, the leader is restored within five minutes of coming back online. If the preferred leader is offline for a very long time, though, it might need additional time to restore its required information from the replica.

There is a small possibility that some messages might be lost when switching back to the preferred leader. You can minimize the chance of lost data by setting the acks property on the Producer to all. See Acknowledgements.

Unclean Leader Election
Enable unclean leader election to allow an out-of-sync replica to become the leader and preserve the availability of the partition. With unclean leader election, messages that were not synced to the new leader are lost. This provides balance between consistency (guaranteed message delivery) and availability. With unclean leader election disabled, if a broker containing the leader replica for a partition becomes unavailable, and no in-sync replica exists to replace it, the partition becomes unavailable until the leader replica or another in-sync replica is back online.

To enable unclean leader election, navigate to Kafka Service > Configuration > Service-Wide. Check the box labeled Enable unclean leader election, click Save Changes, and restart the Kafka service.

Acks = all must be used in conjunction with min.insync.replicas. min.insync.replicas can be set at the broker or topic level. min.insync.replicas = 2 implies that atleast 2 brokers that are ISR must respond that they have data. If you use replication.factor = 3 , min.insync.replicas = 2,acks = all. You can tolerate 1 broker going down , otherwise producer will receive an exception on send.

Acknowledgements
When writing or configuring a Kafka producer, you can choose how many replicas commit a new message before the message is acknowledged using the acks property.

Set acks to 0 (immediately acknowledge the message without waiting for any brokers to commit), 1 (acknowledge after the leader commits the message), or all (acknowledge after all in-sync replicas are committed) according to your requirements. Setting acks to all provides the highest consistency guarantee at the expense of slower writes to the cluster.

Minimum In-sync Replicas
You can set the minimum number of in-sync replicas (ISRs) that must be available for the producer to successfully send messages to a partition using the min.insync.replicas setting. If min.insync.replicas is set to 2 and acks is set to all, each message must be written successfully to at least two replicas. This guarantees that the message is not lost unless both hosts crash.

It also means that if one of the hosts crashes, the partition is no longer available for writes. Similar to the unclean leader election configuration, setting min.insync.replicas is a balance between higher consistency (requiring writes to more than one broker) and higher availability (allowing writes when fewer brokers are available).

The leader is considered one of the in-sync replicas. It is included in the count of total min.insync.replicas. However, leaders are special, in that producers and consumers can only interact with leaders in a Kafka cluster.

To configure min.insync.replicas at the cluster level, navigate to Kafka Service > Configuration > Service-Wide. Set Minimum number of replicas in ISR to the desired value, click Save Changes, and restart the Kafka service.

To set this parameter on a per-topic basis, navigate to Kafka Service > Configuration > Kafka broker Default Group > Advanced, and add the following to the Kafka Broker Advanced Configuration Snippet (Safety Valve) for kafka.properties:
min.insync.replicas.per.topic=topic_name_1:value,topic_name_2:value
Replace topic_name_n with the topic names, and replace value with the desired minimum number of in-sync replicas.

You can also set this parameter using the /usr/bin/kafka-topics --alter command for each topic. For example:
/usr/bin/kafka-topics --alter --zookeeper zk01.example.com:2181 --topic topicname \
--config min.insync.replicas=2
Kafka MirrorMaker
Kafka mirroring enables maintaining a replica of an existing Kafka cluster. You can configure MirrorMaker directly in Cloudera Manager 5.4 and higher.

The most important configuration setting is Destination broker list. This is a list of brokers on the destination cluster. You should list more than one, to support high availability, but you do not need to list all brokers.

MirrorMaker requires that you specify a Topic whitelist that represents the exclusive set of topics to replicate. The Topic blacklist setting has been removed in CDK 2.0 and higher Powered By Apache Kafka.

Note: The Avoid Data Loss option from earlier releases has been removed in favor of automatically setting the following properties. Also note that MirrorMaker starts correctly if you enter the numeric values in the configuration snippet (rather than using "max integer" for retries and "max long" for max.block.ms).
Producer settings
acks=all
retries=2147483647
max.block.ms=9223372036854775807
Consumer setting
auto.commit.enable=false
MirrorMaker setting
abort.on.send.failure=true
Categories: Acknowledgements | Administrators | High Availability | Kafka | MirrorMaker | Preferred Leader Election | All Categories
##############################################################################################################################################################
Apache Kafka, which is a kind of Publish/Subscribe Messaging system, gains a lot of attraction today. We can see many use cases where Apache Kafka stands with Apache Spark, Apache Storm in Big Data architecture which need real-time processing, analytic capabilities.
To integrate with other applications, systems, we need to write producers to feed  data into Kafka and write the consumer to consume the data.  However, Apache Kafka Connect which is  one of new features has been introduced in Apache Kafka 0.9, simplifies the integration between Apache Kafka and other systems. Apache Kafka Connect supports us to quickly define connectors that move large collections of data from other systems into Kafka and from Kafka to other systems. Let’s take a look at the overview of the Apache Kafka Connect:

Apache Kafka Connect - Overview
Apache Kafka Connect – Overview

The Sources in Kafka Connect are responsible for ingesting the data from other system into Kafka while the Sinks are responsible for writing the data to other systems. Note that another new feature has been also introduced in Apache Kafka 0.9 is Kafka Streams. It is a client library for processing and analyzing data stored in Kafka. We can filter, transform, aggregate, the data streams. By combining the Kafka Connect with Kafka Streams, we can build prefect data pipelines.

public class StreamFilterTweets {

psvm{
Properties properties = new Properties();
properties.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG,"127.0.0.1:9092");
properties.setProperty(StreamsConfig.APPLICATION_ID_CONFIG,"demo-kafka-streams");
properties.setProperty(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class.getName());
properties.setProperty(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class.getName());
//create a topology
StreamsBuilder streamsBuilder = new StreamsBuilder();
//input topic 
KStream<String,String> inputTopic = streamsBuilder.sream("twitter_tweets");
KStream<String,String> filteredStream = inputTopic.filter(
           (k,jsonTweet) -> extractUserFollwersIntweet(jsonTweet) >10000 
);
filteredStream.to("important_tweets");
//build the topology
KafkaStreams kafkaStreams = new KafkaStreams(
streamsBuilder.build(),properties
);
//start our streams application
kafkaStreams.start();
}

private static JsonParser jsonParser = new JsonParser();
 private static Integer  extractUserFollwersIntweet(String jsontweet){
 try{
 return jsonParser.parse(jsonTweet).getAsJsonObject().get("user").getAsJsonObject().get("foolowers_count").getAsInt();
 
 }catch(Exception e){}
 }

}

 
2. Some Apache Kafka Connectors
Currently, there are some opensource Sources and Sinks Connectors from community as below:

Connectors	References
Apache Ignite	Source, Sink
Elastic Search	Sink1, Sink2, Sink3
Cassandra	Source1, Source 2, Sink1, Sink2
MongoDB	Source
HBase	Sink
Syslog	Source
MQTT (Source)	Source
Twitter (Source)	Source, Sink
S3	Sink1, Sink2
Or you can find some certified Connectors from Confluent.io via this link

3. Example
3. 1. File Connectors
I’d like to take an example from Apache Kafka 0.10.0.0 distribution and elaborate it. The example is used to demo how to use Kafka Connect to stream data from source which is  file test.txt to destination which is also a file, test.sink.txt. Note that the example will run on the standalone mode.

Apache Kafka Connect Example - FileStream 
Apache Kafka Connect Example – FileStream

 

We will need to use 2 connectors:

FileStreamSource reads the data from the test.txt file and publish to Kafka topic: connect-test
FileStreamSink which will consume data from connect-test topic and write to the test.sink.txt file.
Let’s see configuration file for the Source at kafka_2.11-0.10.0.0\config\connect-file-source.properties

#The name which you give to the kafka connector.It is used for offset management, groupId and so on.
name=local-file-source
#It actually tells kafka connect what class to run as source connector.
connector.class=org.apache.kafka.connect.file.FileStreamSourceConnector
#No of task you run in parallel to complete your job.
tasks.max=1
file=test.txt
topic=connect-test
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=false
value.converter.schemas.enable=false

1
2
3
4
5
name=local-file-source-distributed
connector.class=org.apache.kafka.connect.file.FileStreamSourceConnector
tasks.max=1
file=test-distributed.txt
topic=connect-test-distributed
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=true
value.converter.schemas.enable=true
We need to define the connector.class, the maximum of tasks will we created, the file name that will be read by connector and the topic where data will be published.

Here is the configuration file for the Sink at kafka_2.11-0.10.0.0\config\connect-file-sink.properties

name=local-file-sink
connector.class=FileStreamSink
tasks.max=1
file=test.sink.txt
topics=connect-test
1
2
3
4
5
name=local-file-sink
connector.class=FileStreamSink
tasks.max=1
file=test.sink.txt
topics=connect-test
In similar to the Source, we need to define the connector.class, the number of tasks, the destination file where the data will be written and the topic which data will be consumed.

One important configuration file located at: kafka_2.11-0.10.0.0\config\connect-standalone.properties we need to define the address of the Kafka broker, the keys, values converters.

bootstrap.servers=localhost:9092
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=true
value.converter.schemas.enable=true
...
1
2
3
4
5
6
bootstrap.servers=localhost:9092
key.converter=org.apache.kafka.connect.json.JsonConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=true
value.converter.schemas.enable=true
...
3. 2. Run the example
Make sure you have Apache Kafka 0.9.x or 0.10.x deployed and ready. Assume that we have an Kafka distribution at /opt/kafka_2.11-0.10.0.0. Note that we will use some basic Kafka command line below. If you’re not familiar with those, you can reference another post : Apache Kafka Command Line Interface

3.2.1. Start Kafka broker
We first should cd(change directory) to the Kafka distribution folder.

cd /opt/kafka_2.11-0.10.0.0
1
cd /opt/kafka_2.11-0.10.0.0
Start ZooKeeper

./bin/zookeeper-server-start.sh config/zookeeper.properties &
1
./bin/zookeeper-server-start.sh config/zookeeper.properties &
Start Kafka Server

./bin/kafka-server-start.sh config/server.properties
1
./bin/kafka-server-start.sh config/server.properties
3.2.1. Start the Source and Sink connectors
./bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties
1
./bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties
Note that after this command, the connector is ready for reading the content from test.txt file which should be located in the execution folder: /opt/kafka_2.11-0.10.0.0

3.2.2. Start the Source connector
Write some content to the test.txt file

echo 'hello' >> test.txt
echo 'halo' >> test.txt
echo 'salut' >> test.txt
1
2
3
echo 'hello' >> test.txt
echo 'halo' >> test.txt
echo 'salut' >> test.txt
3.2.3. Check whether the Source connector feed the test.txt content into the topic connect-test or not
 ./bin/kafka-console-consumer.sh  --zookeeper localhost:2181 --from-beginning --topic connect-test
1
 ./bin/kafka-console-consumer.sh  --zookeeper localhost:2181 --from-beginning --topic connect-test
The output on the console is:

{"schema":{"type":"string","optional":false},"payload":"hello"}
{"schema":{"type":"string","optional":false},"payload":"halo"}
{"schema":{"type":"string","optional":false},"payload":"salut"}
1
2
3
{"schema":{"type":"string","optional":false},"payload":"hello"}
{"schema":{"type":"string","optional":false},"payload":"halo"}
{"schema":{"type":"string","optional":false},"payload":"salut"}
3.2.4. Check whether the Sink Connector write content to the test.sink.txt or not
cat test.sink.txt
1
cat test.sink.txt
The output on my console:

hello
halo
salut
1
2
3
hello
halo
salut
4. Conclusions
We have seen the overview of Apache Kafka Connect and an simple example that using FileStream connectors. We can leverage Kafka Connectors to quickly ingest data from a lot of sources, do some processing and write to other destinations. Basically, everything can be done by Apache Kafka, we don’t need to use either other libraries, frameworks like Apache Flume or custom producers. In next posts, I will introduce more about using other types of Kafka Connectors like HDFS sink, JDBC sources, etc and how to implement a Kafka Connector.
#######################################################################################################################################################################################
Configuring Message Retention
Apache Kafka uses Log data structure to manage its messages. Log data structure is basically an ordered set of Segments whereas a Segment is a collection of messages. Apache Kafka provides retention at Segment level instead of at Message level. Hence, Kafka keeps on removing Segments from its end as these violate retention policies.

Apache Kafka provides us with the following retention policies -

Time based Retention
Size based Retention
Time based Retention Policy
Under this policy, we configure the maximum time a Segment (hence messages) can live for. Once a Segment has spanned configured retention time, it is marked for deletion or compaction depending on configured cleanup policy. Default retention time for Segments is 7 days.

Here are the parameters (in decreasing order of priority) that you can set in your Kafka broker properties file:

# Configures retention time in milliseconds
log.retention.ms=1680000

# Used if log.retention.ms is not set
log.retention.minutes=1680

# Used if log.retention.minutes is not set
log.retention.hours=168
Apart from Kafka Broker level configuration, it is also possible to configure retention time by means of Topic level configuration using alter command. E.g.. below command can be used to set retention time as 1680 seconds for a Topic with name my-topic:

./bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic my-topic --config retention.ms=1680000
If required, it is possible to remove Topic level retention time configuration using below command -

./bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic my-topic  --delete-config retention.ms
Note: Topic level configuration will always override Broker level configurations.

 

Size based Retention Policy
In this policy, we configure the maximum size of a Log data structure for a Topic partition. Once Log size reaches this size, it starts removing Segments from its end. This policy is not popular as this does not provide good visibility about message expiry. However it can come handy in a scenario where we need to control the size of a Log due to limited disk space.

Here are the parameters that you can set in your Kafka broker properties file:

# Configures maximum size of a Log
log.retention.bytes=104857600
Apart from Kafka Broker level configuration, it is also possible to configure retention size by means of Topic level configuration using alter command. E.g.. below command can be used to set retention size as 100MB for a Topic with name my-topic:

./bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic my-topic --config retention.bytes=104857600
If required, it is possible to remove Topic level retention time configuration using below command -

./bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic my-topic  --delete-config retention.bytes
Note: Topic level configuration will always override Broker level configurations.


In case of transient failures developers are expected to handle exceptions, otherwise the data will be lost.Example of transient failures = Not enough replicas exception. There is a retries setting, defaults to 0 for kafka <= 2.0 , defaults to 2147483647 for kafka >= 2.1. The retry.backoff.ms setting is by default 100 ms. The producer won't try the request forever it is bounded by a timeout, delivery.timeout.ms = 120000 ms == 2 minutes.
Records will be failed if they can't be acknowledged in delivery.timeout.ms .

In case of retries there is a chance that messages will be sent out of order. If you rely on key based ordering that can be an issue. For this you can configure the setting that controls how many produce requests can be made in parallel: max.in.flight.requests.per.connection default 5 set it to 1 if you want to ensure ordering but it might affect throughput.

Another solution to it is using Idempotent producer.
Producers can introduce duplicate message in kafka due to network errors.In kafka > 0.11 you can define idempotent producer which won't introduce duplicates on network error.
https://issues.apache.org/jira/browse/KAFKA-5494

Idempotent Producers are great to guarantee a stable and safe pipeline , retries = Integer.MAX_VALUE(2147483647) , max.in.flight.requests.per.connection = 1 for kafka <= 0.11 , but max.in.flight.requests.per.connection=5 for kafka > 1. producerProps.put("enable.idempotence",true);

 Map<String, Object> config = new HashMap<>();

        config.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "127.0.0.1:9092");
        config.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        config.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);
		config.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "true");
		config.put(ProducerConfig.ACKS_CONFIG, "all");
		config.put(ProducerConfig.RETRIES_CONFIG, Integer.toString(Integer.MAX_VALUE));
		config.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, "5");
		
		https://blog.cloudflare.com/squeezing-the-firehose/
		
Producer Compression:: 

Producer usually send data that is text-based ,for example with JSON data. In this case it is important to apply compression to the producer. Compression is enabled at the producer level and doesnot require configuration change in brokers or in the consumers."compression.type" can be 'none' (default),'gzip','lz4','snappy'.Compression is more effective the bigger the batch of message being sent to Kafka.

The compressed batch has the following advantage: Much smaller produce request size, faster to transer data over network less latency.Better throughput , better disk utilisation in kafka( Stored messages on disk are smaller ).
Disadvantages:: Producers must commit some CPU cycles to compression , Consumers must commit some CPU cycles to decompression. 
Consider testing snappy or lz4 for optimal speed/ compression ratio.

Batching ::

By default kafka , tries to send records as soon as possible. It will have upto 5 requests in flight meaning upto 5 messages individually at the same time. After this, if more messages have to be sent while others are in flight, kafka is smart and will start batching them while they wait to send them all at once. This smart batching allows kafka to increase throughput while maintaining very low latency, batches have higher compression ratio so better efficiency.

Linger.ms :: No of milliseconds a producer is willing to wait before sending a batch out(default 0). by introducing some lag (linger.ms = 5) we increase the chance of message being sent in batch. So at the expense of introducing some delay we can increase throughput, compression and efficiency of our producer. If batch is full before end of the linger.ms period
it will be sent to kafka right away.
batch.size :: Maximum number of bytes that will be inculded in a batch, default 16 kb. Increasing batch size to something like 32 or 64 kb can help  increasing throughput, compression and efficiency of requests. Any message that is bigger than the batch size will not be batched. A batch is allocated per partition, so make sure you don’t set it too high otherwise you will run waste memory.

batch.size=32KB , linger.ms=20 ms

config.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "snappy");
config.put(ProducerConfig.LINGER_MS_CONFIG, "20");
config.put(ProducerConfig.BATCH_SIZE_CONFIG, Integer.toString(32*1024));

Producer default partitioner and how keys are hashed.
by default keys are hashed using murmur2 algorithm, It is most likely preferred to not override the behaviour of partitioner but is possible to do so Partitioner.Class , The formula is
targetPartition = Util.abs(Utils.murmur2(record.key())) % numPartitions;
This means same key will go to same partition , and adding partitions will alter formula.

Buffer Memory::
If the producer produces faster than the broker can take,the records will be buffered in memory. buffer.memory=33554432(32 MB) : the size of send buffer
If the buffer is full then .send() method will start to block.
max.block.ms=60000 the time .send() will block until throwing an exception.
The producer has filled up its buffer, the broker is not accepting any new data,60 seconds have elapsed.
************************************************************************************************************************************************************************************
GET /_cat/nodes?v ---list of nodes in our cluster.
GET /_cat/indices?v ---List all indices.

String id = record.topic() + "_" + record.partition + "_" + record.offset();
String jsonstr = "{ \"foo\" : \"bar\"}";
IndexRequest indexRequest = new IndexRequest("twitter","tweets",id // this is to make our consumer idempotence
).source(jsonstr,XContentType.JSON);

Delivery Semantics AtLeast Once ::- Offsets are committed after the message is processed. If the processing goes wrong the message will be read again. This can result in duplicate processing of messages, Make sure your processing is idempotent.

Consumer Poll behaviour

Fetch.min.bytes(default 1) : Controls how many data you want to pull at least on each request , helps improving throughput and decreasing request number. At the cost of Latency.
Max.poll.records(default 500) : Controls how many records to receive per poll request. Increase if your messages are small and have lot of RAM.

Max.partitions.fetch.bytes(default 1MB) : Maximum data returned for each request( covers multiple partitions), the consumer perform multiple fetches in parallel.
enable.auto.commit = true & synchronous processing of batches with auto-commit offsets will be committed automatically for you at regular interval.
auto.commit.interval.ms = 5000 default, if you don't use synchronous processing you will be in at-most-once behaviour, because data will be comitted before your data is processed.
 So , enable.auto.commit = false & synchronous processing of data. You control when you commit offsets and whats condition for commiting them.
Example : accumulating records into a buffer and then flushing the buffer to a database + comitting offsets then.
config.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "false"); //disable auto commit of offsets.
config.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, "100");

Integer recordCount = records.count();
if( recordCount > 0 ){
consumer.commitSync();
}

Consumer Offset Reset behaviour :: A Consumer is expected to read from log continuously, if your application has a bug, your consumer can be down, if kafka has a retention of 7 days and your consumer is down for more than 7 days, the offsets are invalid.
The behaviour for the consumer is to then use :: 
auto.offset.reset = latest :- will read from the end of log.
auto.offset.reset = earliest :- will read from the start of log.
auto.offset.reset = none :- will throw exception if no offset is found.

Additionally, consumer offsets can be lost, if consumer hasn't read any new data in 1 day ( Kafka < 2.0) & if a consumer hasn't read new data in 7 days( Kafka > 2.0 ) this can controlled by broker setting offset.retention.minutes .

To replay data for a consumer group , take all the consumers from a specific group down use kafka-consumer-groups command to set offset what you want , restart consumers.
kafka-consumer-groups --bootstrap-server 127.0.0.1:9092 --group <kafka-demo> --describe

Reset offset :: kafka-consumer-groups --bootstrap-server 127.0.0.1:9092 --group <kafka-demo> --reset-offsets --execute --to-earliest --topic twitter-topic
Consumers in a group talk to a consumer group Coordinator , to detect consumers that are down there is a heartbeat mechanism and a poll mechanism. to avoid issues consumers are encouraged to process data fast and poll often.

session.timeout.ms (default 10 seconds ) :: heartbeats are send periodically to broker , if no heartbeat is sent during that period the consumer is considered dead.Set even lower to faster consumer rebalances.
Heartbeat.interval.ms (default 3 seconds) :: how often to send hearbeats , Usually set to 1/3 of session.timeout.ms

max.poll.interval.ms ( default 5 minutes) maximum amount of time between two poll() calls before declaring the consumer thread , this is particularly for big data frameworks like sparks in case the processing takes time.

#######################################################################################################################################################################################
Kafka Connect is all about code and connectors reuse.
More Partition implies:: Each partition can handle throughput of few mb/s , better parallelism better throughput.Ability to run more consumers in a group to scale , Ability to leverage more brokers if you have large cluster , but more elections to perform for ZooKeeper. More files opened on kafka.

Partitions per topic , for small cluster < 6 brokers , 2 * broker_count
For Large cluster < 12 broker . 1 * broker_count.
Adjust for no of consumers you need to run in parallel at peak throughput , adjust for producer throughput( increase if super high throughput).

Replication factor should be atleast 2 , usually 3 maximum 4.
Higher replication factor , more resilience , more disk space required, higher latency (if acks =all).
Broker should not hold more than 2000 - 3000 partition across all topics of that broker. Additionally a kafka cluster should have a maximum of 20,000 partitions across all brokers.
In case broker goes down, zookeeper needs to perform lots of leader election.
If you need more partition in your cluster add brokers. no need to configure topic with 1000 partition for higher throughput.
https://riccomini.name/how-paint-bike-shed-kafka-topic-naming-conventions

It's not easy to set up a cluster , you want to isolate each zookeeper and broker on seperate servers.Monitoring needs to be implemented , Operations have to be mastered.
Alternative :: many different "Kafka as a service" offerings on the web. Like Confluent api.
https://www.datadoghq.com/blog/monitoring-kafka-performance-metrics/
https://kafka.apache.org/documentation/#monitoring
https://docs.confluent.io/current/kafka/monitoring.html

Kafka exposes metrics through JMX, these are highly important for monitoring kafka,ensuring system behave correctly under load.Common places to host kafka metrics ::: 
ELK, Datadog, Newrelic,Confluence Control Center,Promotheus.
Some of the important metrics are :: 
Under Replicated Partitions :: Number of partitions that have problems with ISR may indicate a high load on the system.
Request Handlers :: utilisation of threads for IO, network etc., overall utilisation of an apache kafka borker.
Request Timing :: how long it takes to reply to requests , Lower is better as latency will be improved.

Authentication in kafka can take few forms :::
SSL authentication:: clients authenticate to kafka using ssl certificates.
SASL :: Plain - Clients authenticate using username and password , Kerberos - Microsoft Active Directory , SCRAM - Username / password.
Mirror Maker :: For replicating kafka cluster in a multi cluster environment, Replication preserves data not offsets.
https://kafka.apache.org/documentation/#brokerconfigs 
kafka-configs --zookeeper 127.0.0.1:2181 --entity-type topics --entity-name configured-topic --add-config min.insync.replicas=2 --alter //--delete-config
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
Partitions and Segments :: Topics are made of partitions, partitions are made of segments (files), Only one segment is active (the one data is being written to).
Two Segment settings :: - log.segment.bytes -> max size of a single segment in bytes. log.segment.ms -> the time kafka will wait before committing the segment if not full.
Segment come with two indexes(files) :: An offset to position index : allows kafka where to read to find a message,a timestamp to offset index : allows kafka to find messages with a timestamp therefore kafka knows where to find data in a constant time.
A smaller log.segment.bytes(default 1 gb) means :: More segments per partitions,Log compaction happens more often but kafka has to keep more files opened(Error too many open files).
A smaller log.segment.ms (default 1 week) means :: You set a max frequency for log compaction (more frequent triggers) , you want daily compaction instead of weekly.
min.compaction.lag.ms :: (default 0) how long to wait before message can be compacted
delete.retention.ms :: (default 24hrs) wait before deleting data marked for compaction.
min.cleanable.dirty.ratio :: (default .5) 
how fast will i have new segments based on throughput,how often do i need log compaction to happen.
Many kafka clusters make data expire according to a policy, this concept is called Log Cleanup.
log.cleanup.policy=delete , delete based on age of data(default is 1week),delete based on max size of logs(default -1 to infinite).
log.cleanup.policy=compact,delete based on keys of your messages, will delete old duplicate keys after the active segment is committed.Infinite time and space retention.
Deleting data from kafka allows you to control size of data on disk,delete obselete data.Limit maintenance work on kafka cluster, log cleanup shouldn't happen too often > takes CPU RAM and resources , the cleaner checks for worker every 15 seconds (log.cleaner.backoff.ms)
Log cleanup policy : delete , log.retention.hours=17520 and log.retention.bytes=524288000 (500 mb)
Log compaction ensures that your log contains at least the last known value for a specific key within a partition, the idea is we keep latest upadte for a key in our log.Any consumer that is reading from the tail of log will still see all the messages sent to the topic.Ordering of messages is kept the offset of message is immutable offsets are skipped if message is missing , deleted records can still be seen by consumers for a period of delete.retention.ms (default is 24 hrs). 
You cant trigger log compaction using api call,make sure to assign more memory to get it restarted .
/data/kafka/
If all your insync replicas die you have option to wait for ISR to come back online , enable unclean.leader.election=true and start producing to non ISR partitions.If you enable unclean.leader.election= true. you improve data availability but you will lose data because other messages on ISR will be discarded.
Confluent CLI 
./confluent local start // stop ,for deletion use destroy 
for starting kafka on small machine use,
export KAFKA_HEAP_OPTS="-Xmx256M -Xms128M"
To run kafka from remote machine, change advertised listener to your public ip in config.properties. advertised.listeners
